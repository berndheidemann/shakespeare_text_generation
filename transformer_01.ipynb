{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115393"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us\n"
     ]
    }
   ],
   "source": [
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 46, 1, 24, 53, 56, 42]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_token = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "def encode(text):\n",
    "    return [token_to_idx.get(ch, 0) for ch in text]\n",
    "\n",
    "def decode(text_encoded):\n",
    "    return ''.join([idx_to_token.get(i, '') for i in text_encoded])\n",
    "\n",
    "encode(\"Oh Lord\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh Lord'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([27, 46, 1, 24, 53, 56, 42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
       "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
       "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
       "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
       "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
       "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
       "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
       "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
       "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
       "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
       "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
       "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "data[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7836, 865)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, seq_len, block_size=128):\n",
    "        self.data = data.to(device)\n",
    "        self.seq_len = seq_len\n",
    "        self.block_size = block_size\n",
    "        self.sos_token = torch.tensor([token_to_idx[\" \"]], dtype=torch.long).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data) - self.seq_len) // self.block_size\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x = self.data[i * self.block_size: (i + 1) * self.block_size]\n",
    "        x = torch.cat([self.sos_token, x])\n",
    "        y = self.data[i * self.block_size + 1: (i + 1) * self.block_size + 1]\n",
    "        y = torch.cat([self.sos_token, y])\n",
    "        return x, y\n",
    "    \n",
    "seq_len = 768\n",
    "train_ds = Dataset(train_data, seq_len)\n",
    "val_ds = Dataset(val_data, seq_len)\n",
    "\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 129]) torch.Size([32, 129])\n",
      "xb end: tensor([ 1,  1, 54, 50, 39, 41, 43,  1, 61, 47, 58, 46,  1, 61, 47, 57, 46, 47,\n",
      "        52, 45,  6,  0, 27, 56,  1, 58, 46, 39, 58,  1, 58, 46, 43,  1, 56, 43,\n",
      "        57, 53, 50, 59, 58, 43,  1, 39, 41, 58, 47, 52, 45,  1, 53, 44,  1, 63,\n",
      "        53, 59, 56,  1, 40, 50, 53, 53, 42,  0, 15, 53, 59, 50, 42,  1, 46, 39,\n",
      "        60, 43,  1, 39, 58, 58, 39, 47, 52,  5, 42,  1, 58, 46, 43,  1, 43, 44,\n",
      "        44, 43, 41, 58,  1, 53, 44,  1, 63, 53, 59, 56,  1, 53, 61, 52,  1, 54,\n",
      "        59, 56, 54, 53, 57, 43,  6,  0, 35, 46, 43, 58, 46, 43, 56,  1, 63, 53,\n",
      "        59,  1, 46], device='cuda:0') yb end: tensor([ 1, 54, 50, 39, 41, 43,  1, 61, 47, 58, 46,  1, 61, 47, 57, 46, 47, 52,\n",
      "        45,  6,  0, 27, 56,  1, 58, 46, 39, 58,  1, 58, 46, 43,  1, 56, 43, 57,\n",
      "        53, 50, 59, 58, 43,  1, 39, 41, 58, 47, 52, 45,  1, 53, 44,  1, 63, 53,\n",
      "        59, 56,  1, 40, 50, 53, 53, 42,  0, 15, 53, 59, 50, 42,  1, 46, 39, 60,\n",
      "        43,  1, 39, 58, 58, 39, 47, 52,  5, 42,  1, 58, 46, 43,  1, 43, 44, 44,\n",
      "        43, 41, 58,  1, 53, 44,  1, 63, 53, 59, 56,  1, 53, 61, 52,  1, 54, 59,\n",
      "        56, 54, 53, 57, 43,  6,  0, 35, 46, 43, 58, 46, 43, 56,  1, 63, 53, 59,\n",
      "         1, 46, 39], device='cuda:0')\n",
      "torch.Size([32, 129, 65])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim)\n",
    "        self.encoder_layer = torch.nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=8, batch_first=True)\n",
    "        self.encoder = torch.nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "        self.decoder_layer = torch.nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=8, batch_first=True)\n",
    "        self.decoder = torch.nn.TransformerDecoder(self.decoder_layer, num_layers=1)\n",
    "        self.fc = torch.nn.Linear(embedding_dim, vocab_size)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, src, tgt=None):\n",
    "        src = self.embedding(src) * math.sqrt(self.embedding_dim)\n",
    "        src = self.pos_encoder(src) \n",
    "        memory = self.encoder(src)\n",
    " \n",
    "        if tgt is None:\n",
    "            out = self.decoder(src, memory)\n",
    "            out=torch.nn.functional.relu(out)\n",
    "            out = self.fc(out)\n",
    "            return out\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.embedding_dim)\n",
    "        out = self.decoder(tgt, memory)\n",
    "        out=torch.nn.functional.relu(out)\n",
    "        out=self.dropout(out)\n",
    "        out=self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = SimpleModel(vocab_size, 128).to(device)\n",
    "train_loader=torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "for xb, yb in train_loader:\n",
    "    print(xb.shape, yb.shape)\n",
    "    print(\"xb end:\", xb[-1], \"yb end:\", yb[-1])\n",
    "    out = model(xb, yb)\n",
    "    print(out.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (0.0455), Y (0.0328), g (0.0305), j (0.0304), A (0.0271), \n",
      "s (0.0461), a (0.0382), i (0.0336), u (0.0322), V (0.0294), \n",
      "Y (0.0323), g (0.0311), ! (0.0304), a (0.0268), x (0.0257), \n",
      "tensor([[ 1, 62, 57, 37]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' xsY'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_text(model, text, length):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Initialisieren Sie die Eingabe mit einem Start-Token oder der kodierten Eingabe\n",
    "        input_data = torch.tensor(encode(text), dtype=torch.long, device=device).unsqueeze(0)\n",
    "        \n",
    "        # Bereiten Sie eine leere Sequenz für die Decoder-Eingabe vor (oder verwenden Sie ein SOS-Token)\n",
    "        decoder_input = torch.tensor([[token_to_idx[\" \"]]], dtype=torch.long, device=device)  # SOS_TOKEN ersetzen durch den tatsächlichen Wert\n",
    "\n",
    "        for i in range(length):\n",
    "            # Verwenden Sie die aktuelle Decoder-Eingabe für die Vorhersage\n",
    "            out = model(input_data, decoder_input)\n",
    "            last_token_logits = out[:, -1, :]\n",
    "            last_token_prob = torch.softmax(last_token_logits, dim=-1)\n",
    "            \n",
    "            # print top 5 tokens\n",
    "            values, indices = torch.topk(last_token_prob, 5)\n",
    "            for v, i in zip(values[0], indices[0]):\n",
    "                print(f\"{decode([i.item()])} ({v.item():.4f})\", end=\", \")\n",
    "            print()\n",
    "            predicted_token = torch.argmax(last_token_prob, dim=1).unsqueeze(1)\n",
    "            \n",
    "            # Aktualisieren Sie die Decoder-Eingabe mit dem neu vorhergesagten Token\n",
    "            decoder_input = torch.cat([decoder_input, predicted_token], dim=1)\n",
    "            input_data = torch.cat([input_data, predicted_token], dim=1)\n",
    "            \n",
    "    print(decoder_input )\n",
    "    return decode(decoder_input.cpu().numpy()[0])  # Ignorieren Sie das SOS-Token bei der Decodierung\n",
    "\n",
    "generate_text(model, \"Oh Lord\", 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0.1110), e (0.0334), s (0.0306), t (0.0279), , (0.0265), \n",
      "  (0.1124), e (0.0346), s (0.0308), t (0.0283), , (0.0271), \n",
      "  (0.1111), e (0.0353), s (0.0307), t (0.0285), , (0.0276), \n",
      "  (0.1094), e (0.0356), s (0.0306), t (0.0287), , (0.0279), \n",
      "  (0.1079), e (0.0357), s (0.0305), t (0.0289), , (0.0280), \n",
      "  (0.1067), e (0.0358), s (0.0304), t (0.0290), , (0.0281), \n",
      "  (0.1055), e (0.0359), s (0.0303), t (0.0290), , (0.0282), \n",
      "  (0.1045), e (0.0359), s (0.0302), t (0.0291), d (0.0283), \n",
      "  (0.1036), e (0.0359), s (0.0301), t (0.0291), d (0.0285), \n",
      "  (0.1029), e (0.0359), s (0.0301), t (0.0291), d (0.0287), \n",
      "  (0.1022), e (0.0359), s (0.0300), t (0.0292), d (0.0289), \n",
      "  (0.1016), e (0.0358), s (0.0299), t (0.0292), d (0.0291), \n",
      "  (0.1011), e (0.0358), s (0.0298), t (0.0292), d (0.0292), \n",
      "  (0.1006), e (0.0358), s (0.0298), d (0.0293), t (0.0293), \n",
      "  (0.1001), e (0.0358), s (0.0297), d (0.0294), t (0.0293), \n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n",
      "Epoch 0, train_loss: 2.664100306649362, val_loss: 1.3629016365323747 sample_output:                 \n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "model = SimpleModel(vocab_size, 1024)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "def train_epoch(model, train_loader, loss_func, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(xb, yb)\n",
    "        loss = loss_func(y_pred.view(-1, vocab_size), yb.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss/len(train_loader)\n",
    "\n",
    "def validate_epoch(model, val_loader, loss_func):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            y_pred = model(xb, yb)\n",
    "            loss = loss_func(y_pred.view(-1, vocab_size), yb.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss/len(val_loader)\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "val_loader=torch.utils.data.DataLoader(val_ds, batch_size=128, shuffle=False)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for i in range(5000):\n",
    "    train_loss = train_epoch(model, train_loader, loss_func, optimizer)\n",
    "    train_losses.append(train_loss)\n",
    "    if i % 10 == 0:\n",
    "        val_loss = validate_epoch(model, val_loader, loss_func)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f'Epoch {i}, train_loss: {train_loss}, val_loss: {val_loss} sample_output:', generate_text(model, \"Oh Lor\", 15))\n",
    "    else:\n",
    "        val_losses.append(val_losses[-1])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(val_losses, label='val loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thank youuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, 'Thank you', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a strange repose, to be asleep With eyes wide open; standing, speaking, moving,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, \"This is a strange repose, to be asleep With eyes wide open; standing, speaking, moving,\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.0042, Val Loss: 0.0023\n",
      "Epoch 1, Train Loss: 0.0031, Val Loss: 0.0018\n",
      "Epoch 2, Train Loss: 0.0026, Val Loss: 0.0015\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    train_loss = train_epoch(model, train_loader, loss_func, optimizer)\n",
    "    train_losses.append(train_loss)\n",
    "    if i % 10 == 0:\n",
    "        val_loss = validate_epoch(model, val_loader, loss_func)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f'Epoch {i}, train_loss: {train_loss}, val_loss: {val_loss}')\n",
    "    else:\n",
    "        val_losses.append(val_losses[-1])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(val_losses, label='val loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model, 'Thank you', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a strange repose, to be asleep With eyes wide open; standing, speaking, moving,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, \"This is a strange repose, to be asleep With eyes wide open; standing, speaking, moving,\", 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
